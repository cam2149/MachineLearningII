{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67937baa",
   "metadata": {},
   "source": [
    "## Ejemplos de ensambles\n",
    "\n",
    "A continuación procedemos con un ejemplo básico de bagging para árboles de regresión, en el que comparamos el estimador del árbol de decisión con el estimador bagging correspondiente. Usamos la métrica R2 (coeficiente de determinación) para comparar los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9840b60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeRegressor R^2 score =  0.5676142169432943 \n",
      "Bagging R^2 score =  0.8040380122022653 \n",
      "Bagging OOB R^2 score =  0.8208564723226458\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Bagging Example \"\"\"\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "np.random.seed(100)\n",
    "\n",
    "# create regression problem\n",
    "n_points = 1000 # points\n",
    "x, y =  make_friedman1(n_samples=n_points, n_features=15, \n",
    "                       noise=1.0, random_state=100)\n",
    "\n",
    "# split to train/test set\n",
    "x_train, x_test, y_train, y_test = \\\n",
    "        train_test_split(x, y, test_size=0.33, random_state=100)\n",
    "\n",
    "# training\n",
    "regTree = DecisionTreeRegressor(random_state=100)\n",
    "regTree.fit(x_train,y_train)\n",
    "\n",
    "# test\n",
    "yhat = regTree.predict(x_test)\n",
    "\n",
    "# Bagging construction\n",
    "n_estimators=500\n",
    "bag = np.empty((n_estimators), dtype=object)\n",
    "bootstrap_ds_arr = np.empty((n_estimators), dtype=object)\n",
    "for i in range(n_estimators):\n",
    "    # sample bootsraped dataset\n",
    "    ids = np.random.choice(range(0,len(x_train)),size=len(x_train), replace=True)\n",
    "\n",
    "    x_boot = x_train[ids]\n",
    "    y_boot = y_train[ids]\n",
    "    bootstrap_ds_arr[i] = np.unique(ids)\n",
    "    \n",
    "    bag[i] = DecisionTreeRegressor()\n",
    "    bag[i].fit(x_boot,y_boot)\n",
    "\n",
    "# bagging prediction\n",
    "yhatbag = np.zeros(len(y_test))   \n",
    "for i in range(n_estimators): \n",
    "    yhatbag = yhatbag + bag[i].predict(x_test)\n",
    "        \n",
    "yhatbag = yhatbag/n_estimators\n",
    "\n",
    "# out of bag loss estimation\n",
    "oob_pred_arr = np.zeros(len(x_train))\n",
    "for i in range(len(x_train)):\n",
    "    x = x_train[i].reshape(1, -1)\n",
    "    C = []\n",
    "    for b in range(n_estimators):\n",
    "        if(np.isin(i, bootstrap_ds_arr[b])==False):\n",
    "            C.append(b)\n",
    "    for pred in  bag[C]:       \n",
    "        oob_pred_arr[i] = oob_pred_arr[i] + (pred.predict(x)/len(C))        \n",
    "\n",
    "L_oob = r2_score(y_train, oob_pred_arr)\n",
    "\n",
    "print(\"DecisionTreeRegressor R^2 score = \",r2_score(y_test, yhat),  \n",
    "      \"\\nBagging R^2 score = \", r2_score(y_test, yhatbag),\n",
    "      \"\\nBagging OOB R^2 score = \",L_oob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b46b16",
   "metadata": {},
   "source": [
    "Vamos ahora a observar un ensamble con Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94ec21e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF R^2 score =  0.8098070870916778 \n",
      "RF OOB R^2 score =  0.8254584737683683\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Bagging Example RF \"\"\"\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# create regression problem\n",
    "n_points = 1000 # points\n",
    "x, y =  make_friedman1(n_samples=n_points, n_features=15, \n",
    "                       noise=1.0, random_state=100)\n",
    "# split to train/test set\n",
    "x_train, x_test, y_train, y_test = \\\n",
    "        train_test_split(x, y, test_size=0.33, random_state=100)       \n",
    "rf = RandomForestRegressor(n_estimators=500, oob_score = True, max_features=8,random_state=100)\n",
    "rf.fit(x_train,y_train)\n",
    "yhatrf = rf.predict(x_test)\n",
    "\n",
    "print(\"RF R^2 score = \", r2_score(y_test, yhatrf), \n",
    "      \"\\nRF OOB R^2 score = \", rf.oob_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89b0521",
   "metadata": {},
   "source": [
    "En este siguiente ejemplo utilizamos Gradient Boosting para regresión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20bad95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting R^2 score =  0.8992706169055638\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Gradient Boosting Regressor \"\"\"\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# create regression problem\n",
    "n_points = 1000 # points\n",
    "x, y =  make_friedman1(n_samples=n_points, n_features=15, \n",
    "                       noise=1.0, random_state=100)\n",
    "\n",
    "# split to train/test set\n",
    "x_train, x_test, y_train, y_test = \\\n",
    "        train_test_split(x, y, test_size=0.33, random_state=100)\n",
    "\n",
    "# boosting sklearn\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "breg = GradientBoostingRegressor(learning_rate=0.1, \n",
    "            n_estimators=100, max_depth =3, random_state=100)\n",
    "breg.fit(x_train,y_train)\n",
    "yhat = breg.predict(x_test)\n",
    "print(\"Gradient Boosting R^2 score = \",r2_score(y_test, yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407be835",
   "metadata": {},
   "source": [
    "Finalmente observamos un ejemplo con AdaBoost para clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23de313d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" AdaBoost Example \"\"\"\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import zero_one_loss\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def ExponentialLoss(y,yhat):\n",
    "    n = len(y)\n",
    "    loss = 0\n",
    "    for i in range(n):\n",
    "        loss = loss+np.exp(-y[i]*yhat[i])\n",
    "    loss = loss/n\n",
    "    return loss\n",
    "\n",
    "# create binary classification problem\n",
    "np.random.seed(100)\n",
    "\n",
    "n_points = 100 # points\n",
    "x, y =  make_blobs(n_samples=n_points, n_features=5,  centers=2,\n",
    "                      cluster_std=20.0, random_state=100)\n",
    "y[y==0]=-1  \n",
    "\n",
    "# AdaBoost implementation\n",
    "BoostingRounds = 1000\n",
    "n = len(x)\n",
    "W = 1/n*np.ones(n)\n",
    "\n",
    "Learner = []\n",
    "alpha_b_arr = []\n",
    "\n",
    "for i in range(BoostingRounds):\n",
    "    clf = DecisionTreeClassifier(max_depth=1)\n",
    "    clf.fit(x,y, sample_weight=W)\n",
    "    \n",
    "    Learner.append(clf)\n",
    "    \n",
    "    train_pred = clf.predict(x)\n",
    "    err_b = 0\n",
    "    for i in range(n):\n",
    "        if(train_pred[i]!=y[i]):\n",
    "            err_b = err_b+W[i]\n",
    "    err_b = err_b/np.sum(W)\n",
    "        \n",
    "    alpha_b = 0.5*np.log((1-err_b)/err_b)\n",
    "    \n",
    "    alpha_b_arr.append(alpha_b)\n",
    "    \n",
    "    for i in range(n):\n",
    "        W[i] = W[i]*np.exp(-y[i]*alpha_b*train_pred[i])        \n",
    "    \n",
    "yhat_boost = np.zeros(len(y))\n",
    "\n",
    "for j in range(BoostingRounds):\n",
    "    yhat_boost = yhat_boost+alpha_b_arr[j]*Learner[j].predict(x)\n",
    "    \n",
    "    \n",
    "yhat = np.zeros(n)\n",
    "yhat[yhat_boost>=0]=1\n",
    "yhat[yhat_boost<0]=-1\n",
    "print(\"AdaBoost Classifier exponential loss = \", ExponentialLoss(y, yhat_boost)) \n",
    "print(\"AdaBoost Classifier zero-one loss = \", zero_one_loss(y,yhat) ) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
